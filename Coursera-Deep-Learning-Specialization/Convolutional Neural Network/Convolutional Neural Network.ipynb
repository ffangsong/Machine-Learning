{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of a CNN\n",
    "## Convolution layer\n",
    "- One of the challenges of computer vision problems is that the input image can get really big, Thus in standard fully connected NN, the weight matrix has very large dimension. With so many parameters, it's difficult to get enough data to prevent a NN from overfitting. And also, the computational requirements and the memory requirements to train a NN is a bit infeasible. Thus, for computer vison to use large images, we need to implement the convolution operation.\n",
    "- In general, if a  $n\\times n$ matrix convolved with $f \\times f$ filter/kernel gives us $(n-f+1) \\times (n-f+1)$ matrix \n",
    "- In python, use function conv-forward; In tensorflow use tf.nn.conv2d; in keras use Conv2d function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- We want to apply convolution operation multiple times, but if the image shrinks we will lose a lot of data during this process. Also, the edges pixels are used less than other pixels in an image.\n",
    "- To solve these problems, we can pad the input image before convolution by adding some rows and columns to it. We will call the padding amount $p$ the number of row/columns that we will insert in top, bottom, left and right of the image. \n",
    "- The general rule now, if a matrix $n\\times n $ is convolved with $f\\times f $ filter/kernel and padding $p$ gives us $n+2p-f+1 \\times n+2p-f+1$ matrix. \n",
    "- In computer vision f is usually odd. Some of the reasons is that it will have a center value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride Convolution\n",
    "- When we are making the convolution operation we used $S$ to tell us the number of pixels we will jump when are convolving filter/kernel. \n",
    "- General rule: If a matrix $n\\times n $ is convolved with $f \\times f $ filter/kernel and padding $p$ and stride $s$ it gives us $(\\frac{n+2p-f}{s} +1) \\times(\\frac{n+2p-f}{s} +1) $ matrix\n",
    "- In case $\\frac{n+2p-f}{s} +1$ is fraction we can take floor of this value.\n",
    "- In math textbooks the conv operation is flipping the filter before using it. What we were doing here is called cross- correlation operation, but the state of art of deep learning is using this as conv operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions over volumes\n",
    "- A image of height , width, # of channels is convoluting with  a filter of height, width, and same # of channels. The image number channels and the filter number of channels are the same. The output is here is only 2D. \n",
    "- Different layers/channels in the filter can either be the same matirx or different.  \n",
    "- You can use several  volume filters in one step to detect different features, each of them is going to give a 2D matrix, and then stack the results together to form a 3D matrix. Thus, if a $n\\times n \\times n_c$ matrix convolves with $ n_c'$  filters with shape $f \\times f \\times n_c$, then we got a $(n-f) \\times (n-f) \\times n_c'$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "- CNN often uses pooling layers to reduce the size of the inputs, speed up computation, and to make some of the feature dectors more invariant to its positon in the iinput.  \n",
    "- The two types of pooling layers are: max pooling and average pooing. Max pooling is used much more often than average pooling. \n",
    "- The max pooling is saying, if the feature is detected anywhere in this filter then keep the highest number. But the main reason why people are using pooling is because it works well in practice and reduce computations, and nobody know exactly why pooling works.\n",
    "- Max pooling: slides an (f,f) window over the input and store the max value of the window in the output. Pooling is done on each layer/ chanel independently. \n",
    "- Just  like convolution, f and s are hyperparameters of pooling layer. Padding is rarely used. Most often, s=2, f=2, which is going to shrink the matrix size be a factor around 2. \n",
    "\n",
    "- Poolign layer has no parameters for backprop to train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One layer of convolutional network\n",
    "- We first convolve input with some filters and then add a bias to each filter, and then get RELU activation of the result. The filter plays the similar role as the weight matrix.\n",
    "- Summar of notation:\n",
    "    If layer l is a convolution layer:\n",
    "        - f[l] = filter size in layer l\n",
    "        - p[l] = padding \n",
    "        - s[l] = stride \n",
    "        - n_c[l] = number of filters\n",
    "        - Each filter is f[l] * f[l]* n[l-1]_c\n",
    "        \n",
    "        - Input: n^[l-1]_H * n^[l-1]_w * n^[l-1]_c\n",
    "        - Output: n^[l]_H * n^[l]_w * n^[l]_c$\n",
    "        - n^[l] _H = {n[l-1] + 2p[l] -f[l]}/s[l] +1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why convolutions\n",
    "- Two main advantages of Conv are:\n",
    "    - Parameter sharing\n",
    "        - A feature detector that's useful in one part of the image is probably useful in another part of the image\n",
    "    - Sparsity of connections\n",
    "        - In each layer, each ouput value depends only on a small number of inputs which makes it translation\n",
    "invariance.\n",
    "    Through these two mechanisms, a neural netowrk has a lot fewer parameters which allows it to be trained with smaller training cells and is less prone to be overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep convolutional models: case studies\n",
    "- Here are some classical CNN networks:\n",
    "    - LeNet-5\n",
    "    - AlexNet\n",
    "    - VGG\n",
    "- The best CNN architecture that won the last ImageNet competition is called ResNet and it has 152 layers. \n",
    "- There are also an architecture called Inception that was made by Google that are very useful to learn and apply to your tasks.\n",
    "  <img src=\"../Images/LeNet5.png\" />\n",
    "### Lenet-5\n",
    "- Invented by Young Lecun in 1998.\n",
    "- Some statistics about the example: \n",
    "    -n_c: # of filters\n",
    "\n",
    "\n",
    "|  Layer |                          |Output/Activation Shape |  # of parameters| \n",
    "|------  |-------                   |-------| \n",
    "| Input |                           |(32,32,3) | |\n",
    "|1|CONV1(f1=5,s1=1, p1=0,n_c = 6)| (28,28,6)|156|\n",
    "|  |MaxPooling(f1p =2, s1p=2)       | (14,14,6)|0|\n",
    "| 2 |CONV2(f2=5,s2=1,p2=0, n_c = 16)|  (10,10,16)|416|\n",
    "|    |MaxPooling(f2p = 2, s2p =2) | (5,5,16)|0|\n",
    " | 3| FC3(number of neurons 120)  |(120,1) |48001|\n",
    " | 4 | FC4(number of neurons 84)   | (84,1))|10081|\n",
    " |5| Softmax| (?,1)|?| |\n",
    "### Alexnet\n",
    "- The goal for the model was the ImageNet challenge which classfies images into 1000 classes. \n",
    "- Summary:\n",
    "    ``` Conv => Max-pool => Conv => Max-pool => Conv => Conv => Conv => Max-pool => Flatten => FC=> FC => Softmax\n",
    "    ```\n",
    "- The paper convinced the computer science researchers that deep learning is so important. \n",
    "###VGG-16\n",
    "- A modificatio of AlexNet\n",
    "- Focus on having only these blocks:\n",
    "    - CONV = 3x3 filters, s=1,same padding\n",
    "    - MAX-POOL = 2x2, s=2\n",
    "- Pooling was the only one who is responsible for shrinking the dimensions.\n",
    "## Residual Network(ResNets)\n",
    "- The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges(at the lower level) to very complex features(at the deeper layers). However, using a deeper network doen't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearly slow.More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero(or, in rare cases,grow exponentially quickly and explode to take very large values).\n",
    "- This problem can be solved by using a ResNet.In ResNets, a \"shortcut\" or a \"skip connection\" allow the gradient to be directly backpropogated to earlier layers. \n",
    "- Very, very deep NNs are difficult to train because of vanishing and exploding gradients problems. \n",
    "- In ResNets, we skip connection which makes you take the activation from one layer and suddently feed it to another layer even much deeper in NN which allows you to train deeper NNs even with layers greater than 100. \n",
    "- Theoratically, deeper and deeper NN should lead to smaller and smaller training error, but in practice, because of the vanishing and exploding gradients problems that performance of the network suffers as it goes deeper. While ResNets allows for deeper NN while not hurting the performance. \n",
    "- Two main types of locks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different.\n",
    "    - The identity block: Applies to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation(say $a^{[l+2]}$)\n",
    "    - The convolutional block: applies to the case where the input and output dimensions don't match up. The different with the identity block is that there is a CONV2D layer in the shortcut path.  The CONV2D layer is used to resize the input x to a different dimension, so  that the dimensions match up in the final addition needed to add teh shortcut value back to the main path. \n",
    "        \n",
    "### Why ResNets work?\n",
    "- Identity function is easy for a residul block to learn, which means that adding these two layers/residual block in your NN, it doesn't really hurt yor neural network's ability to do as well as the simpler network without these two layers. Also, if all these residual blocks actually learned something useful then maybe you do even better. \n",
    "- Also, what goes wrong in very  deep palin nets without these residual blocks is that when you make the network deeper and deeper, it's actually very difficutlt for it to choose parameters that learn even the identity function which is why a lot of layers end up making your result worse. \n",
    "- The main reason that residul network works is that it's so easy for these extra layers to learn the identity function that you're kind of guaranteed that it doesn't hurt performance and then a lot of the time you maybe be lucky and then even helps performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 X 1 convolutions/ Network in Network\n",
    "- The idea of one by one convolution has been very influential.For example:\n",
    "    - To shrink the number of channels and therefore save on computations, this is also called feature transformation, while pooing layer only shring $n_w, n_h$\n",
    "    - If you want to keep the number of channels, that's fine too. The effect of the 1X1 convolution is just adds non-linearity. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception network\n",
    "- When designing a CNN, you have to deide all the layer, such as will you pick a 3x3 Conv or 5x5 Conv or maybe a maxplooling layer. You have so many choices.\n",
    "- What inception tells us is, why not use all of them at once? Do the pooing, convs and then stack them together to form a volume output. \n",
    "- But the computation is costy in Inception model, but we can use a 1x1 convolution to reduce the computation. The 1x1 Conv here is called BottleNeck. \n",
    "- It turns out that 1X1 convolution won't hurt the performance. \n",
    "- Inception module\n",
    "\n",
    "- Inception network is the inception module repeatd several times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transfer Learning\n",
    "- If you are using a specific NN architecture that has been trained before, you can use this pretrained parameters instead of random initialization to solve your problems. \n",
    "- Frameworks have options to make the parameters frozen in some layers using ```trainable = 0 ``` or ```freeze =0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "- The more data you have, the better your deep NN's performance. Data augmentation is one of the techniques that deep learning uses to improve the performance of deep NN. \n",
    "- Some data augumentatio methods that are used for CV tasks includes:\n",
    "    - Mirroring\n",
    "    - Random cropping\n",
    "        - The issue with this technique is that you might take a wrong crop, and the solution is to make your crop big enough .\n",
    "    - Color shifting\n",
    "        - For example, we add tO R,G and B channels different distortions that will make the image identified as the same for the human but is different for the computer. \n",
    "        - In practice, the added value are pulled from some probability distribution and these shifts are quite small. \n",
    "        - Makes your algorithm more robust in changing colors in image. \n",
    "        - There are an algorithm which is called PCA color augmentation that decides the shifts needed automatically, and it is given in AlexNet paper. \n",
    "    - Rotation\n",
    "    - Shearing\n",
    "    - Local warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of Computer Vision\n",
    "- Speech recognition problems for example has a big amount of data, while image recognition has a medium amount of data and the object detection has a small amount of data nowadays.\n",
    "- If your problem has a large amount of data, researchers are tend to use:\n",
    "    - Simpler algorithms.\n",
    "    - Less hand engineering\n",
    "- If you don't have that much data people tend to try more hand engineering for the problem \"Hacks\". Like choosing a more complex NN architecture\n",
    "- Learning algorithms has two sources of knowledge:\n",
    "        - (x,y)labels \n",
    "        - hand engineered features/network architectures/other components. \n",
    "- Tips for doing well on benchmarks/winning competitions:\n",
    "    - Ensembling\n",
    "        - Train several networks independently and average their outputs. \n",
    "\n",
    "        - This can give you a push by 2%\n",
    "        - But this will slow down your production by the number of the ensembles. Also it takes more memory as it saves all the models in the memory.\n",
    "        - People use this in competitions but few uses this in a real production.\n",
    "    - Multi crop at test time\n",
    "        - Do data augumentation on test data as well \n",
    "        - Run classifier on multiple versions of test versions and average results.\n",
    "    - Use open source code\n",
    "         - Use architectures of networks published in the literature.\n",
    "         - Use open source implementations if possible.\n",
    "          - Use pretrained models and fine-tune on your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras tutorial:\n",
    "- Define a function to describe your model, remember to creast a model instance in your function, you'll use this instance to train/test the model\n",
    "- To train and test an model in Keras, there are four steps: Create-> Compile -> Train -> Test\n",
    " \n",
    "    - 1. Create the model by calling the function defined.\n",
    "    - 2. Compile by calling model.compile(optimizer = \"\", loss = \"\", metrics= \"\")\n",
    "    - 3. Train the model on train data by calling model.fit(x= ..,y=.., epochs = ..., batch_size= ...)\n",
    "    - 4. Test the model on test data by calling model.evaluate(X=.., Y=...)\n",
    "- Two other basic features of Keras that are useful:\n",
    "     - model.summary(): print the details of your layers in a table with the size of its inputs/outputsï¼Œ and number of parameters at each layer.\n",
    "     - plot_model(): plots your graph in a nice layout. You can even save it as \".png\" using SVG() \n",
    "- To choose the Keras backend you should go to ```$HOME/.keras/keras.json ``` and change the file to the desired backend like Theano or Tensorflow or whatever backend you want. \n",
    "- After you create the model you can run it in a tensorflow session without compling, training and testing capabilities. \n",
    "- You can save your model with ```model_save``` and load your model using ``` load_model```. This will save your whole trained model to disk with the trained weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "## Object Localization\n",
    "\n",
    "- What are localization and detection?\n",
    "    - Image Classification:\n",
    "        - Classify an image to a specific class. The whole image represents one class. We don't want to know exactly where are the object. Usually only one object is presented. \n",
    "    - Classification with localization\n",
    "        - Given an image we want to learn the class of the image and where are the class location in the image. We need to detect a class and a rectangle of where that object is. Usually one object is presented.\n",
    "    - Object detection\n",
    "         - Given an image we want to detect all the object in the image that belong to a specific classes and give their location. An image can contain more than one object with different classses. \n",
    "    - Semantic Segmentation\n",
    "         - We want to Label each pixel in the image with a category label. Semantic Segmantation don't differentiate instances, only care about pixels. It detects no objects just pixels. \n",
    "         - If there are two objects of the same class is intersected, we won't be able to separate them .\n",
    "- To make classificaton with localization we use a Conv Net with a softmax attached to the end of it and a four numbers ```bx, by,bh and bw``` to tell you the location of the class in the image. \n",
    "- Target label Y in classfication with localization problems:\n",
    "    ```\n",
    "    Y = [  Pc                      # probability of an obejct is presented\n",
    "           bx                      # x- coordinate of the center of the Bounding box\n",
    "           by\n",
    "           bh\n",
    "           bw\n",
    "           c1                      # The classes\n",
    "           c2\n",
    "           ...\n",
    "        ]\n",
    "        ```\n",
    "- In practice, we use log likely hood loss for classes and square error for the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Detection\n",
    "- In some of the computer vision problems we will need to output some points. This is called landmark detection.\n",
    "- For example, if you are working in a face recognition problem you might want some points on the face like corners of the eys, corners of the mouth, and corners of the nose and so on. This can help in a lot of application like detecting the pose the person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obejct Detection\n",
    "- We will use a Conv net to solve the object detection problem using a technique called the sliding windows detection algorithm. \n",
    "- For example, we are working on Car detection algorithm.\n",
    "- The first thing, we will train a Conv net on label training set with closely cropped car images(meaning x is pretty much only the car) and non car images\n",
    "- After we finish training of this Conv net we will then use it with the sliding window technique\n",
    "- Sliding window detection algorithm\n",
    "        \n",
    "     - Decide a rectangle size\n",
    "     - Split your image into rectangles of the size you picked. Each region should be covered. You can use some strides as well.\n",
    "     - For each rectangle feed the image into the Conv net and decide if its a car or not\n",
    "     - Pick larger/ smaller rectangles and repeat the process from 2 to 3. The hope is that so long as there's a car somewhere in the image, there will be a window to localize the car.\n",
    "     - Store the rectangles that contains the cars\n",
    "     - If two or more rectangles intersects choose the rectangle with the best accurary.\n",
    "- Disadvantages of sliding window is the computation time\n",
    "- In the era of machine learning before deep learning, people used a hand crafted linear classifiers that classifies the object and then use the sliding window technique. The linear classier make it a cheap computation. But in the deep learning era that is so computational expensive due to the complexity of the deep learning model.\n",
    "- The problem of compution cost has an solution, the sliding window object detection can be implemented using convolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Implementation of Sliding Windows\n",
    "## FC layer can be turned into convolution layers. \n",
    "- A FC layer can be turned into Conv layer using a filter with the  same width and height as the input.  Mathenatically, this is the same as a fully connected layer. \n",
    "\n",
    "## Convolution Implementation of Sliding windows\n",
    "- By turning into convolution, it can make all the predictions of all windows at the same time by one forward pass through the big Conv net.\n",
    "- It's more efficient because it now shares the computations. \n",
    "- The weakness of the algorithm is that the position of the rectangle won't be too accurate. Maybe none of the rectangles is match perfectly with the position of the object you want to recgonize.\n",
    "\n",
    "## Bounding box Predictions\n",
    "- A better algorithm than the sliding window is the YOLO algorithm.\n",
    "- YOLO stands for you only look at once and was developed in 2015.\n",
    "- The basic idea is : \n",
    "    - Lets say we have an image \n",
    "    - Place a nxn grid on the image, in practice, it's a fine gride, say 19X19, to avoid different object were assigned to the same cell.\n",
    "    - Apply the classfication and localization algorithm to each section of the grid. Each grid cell will have a label with the format ```[Pc, bx,by,bw,bh,c1,c2,c3]``` The yolo algorithm take the midpoint of each the object in the image and assign the object to the grid cell containing the midpoint. \n",
    "    - $b_x,b_y, b_w,b_h$ are specified relative to the grid cell. $b_x,b_y$ must be less 1, while $b_w,b_h$ can be greater than 1. \n",
    "    \n",
    "## Techniques that will make yolo work better    \n",
    "### Intesection over Union\n",
    "- It is a function used to evaluate the object detection algorithm.\n",
    "- It computes size of intersection and divide it by the union of output bounding box and ground true boudning box. More generally, IOU is a measur eof the overlap between two bounding boxes. \n",
    "- If your IOT >= 0.5, then its good, the best answer will be 1. \n",
    "- The higer the IOU the better is the accuracy.\n",
    "\n",
    "### Non max suppresion\n",
    "- One of the problems of object detection is that your algorithm rather than detecting an object just once, it might detect it multiple times. Non-max suppresion is a way for you to make sure that your algorithm detects each object only once. \n",
    "- Non max means that you're going to output your maximal probabilies classfications but suppres the close_by ones that are non-maximal.\n",
    "- Algorithm:\n",
    "    - Get the output of each of the grid box. \n",
    "    - Discard all boxes with $P_c < 0.6$\n",
    "    - While for the remaining boxes\n",
    "        - Pick the box with the largest Pc output that as a prediction\n",
    "        - Discard any remaining box with $IOU>0.5$ with that box output in the previous step. \n",
    "    - Repeatly doing the previous step until you've taken each of the boxes and either output it as a prediction, or discard it as having too high IOU.\n",
    " - If you tried to detect multiple, say three objects, you should independently carry out non-max suppression three times, one of each of the outputs classes.\n",
    " \n",
    "## Anchor boxes\n",
    "- One of the problems with object detection as we have seen so far is that each of the grid cells can detect only one object, what if a grid cell wants to detect multiple objects?  \n",
    "- For example, a person standing in front of a car, the center of the car and the person are almost the same position and both of them fall into the same grid cell, the algorithm we saw before will not give two detections.  \n",
    "- To solve this problem,We can use the idea of anchor boxes. \n",
    "- With the idea of anchor boxes, we are going to choose multiple different pre-defined shpaes, and associate multiple predictions with the multiple anchor boxes.  \n",
    "- Previously, each object in training image is assigned to grid cell that contains that object's midpoint. With anchor boxes, each object in training image is assigned to grid cell (that contains object's midpoint) and anchor box with highest IOU. \n",
    "- For example, if we have two anchor box, the target label with be $P_c,P_x,p_y,p_w,p_h, c1,c2,c3 $ associated with anchor box 1, and then another set of output associated withe anchor box 2. \n",
    "- anchor box essentialy refer to a set of values$P_c,P_x,p_y,p_w,p_h, c1,c2,c3 $,  your output will have a dimension which is  8 * number of anchor boxes\n",
    "- You may use k-mean clustering to choose anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO algorithm\n",
    "- YOLO900 Better, faster, stronger\n",
    "- YOLO implenentation can be found here: \n",
    "    - https://github.com/allanzelener/YAD2K\n",
    "    - https://github.com/thtrieu/darkflow\n",
    "    - https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Proposal\n",
    "- Is another set of idea in object detection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Applications of CNN: face recogization and Neural Style Transfer\n",
    "### Face recognization\n",
    "- Face verification vs. face recognization\n",
    "    - Verification \"Is this the claimed person\"\n",
    "        - Input: Image and name/ID\n",
    "        - Output: whether the input image is that of the claimed person\n",
    "    - Recogization \"Who is this person\"\n",
    "        - Has a database of K persons\n",
    "        - Get an input image\n",
    "        - Output ID if the image is any of the K persons\n",
    "- We can use a face verification system as block to build a face recognization system.\n",
    "\n",
    "###  One shot learning\n",
    "- One of the face recognition challenges is to solve one shot learning problem, which means to recognize a person using one image.\n",
    "- Historically, deep learning doesn't work well with a small number of data\n",
    "- Instead, we will learn a similarity function:\n",
    "    - d(img1, img2) = degree of difference between images\n",
    "    - We want d to be low in case of the same faces\n",
    "    - We use tau T as a threshold for d: \n",
    "        - If d(img1, img2) < = T then the faces are the same\n",
    "- Similarity function helps us solving the one shot learning, also its robust to new inputs.\n",
    "\n",
    "### Siamese Network\n",
    "- The simililarity function can be implemented using a type of CNN called Siamese Network.\n",
    "- Siamese Network output an feature vector as an encoding of  the input image.\n",
    "- ``` d(img1, img2) = || feature_vector(img1) - feature_vector(img2)||^2```\n",
    "\n",
    "### Triplet Loss\n",
    "-  Is one of the loss function we can use to learn the parameters in Siamese Network\n",
    "- Our learning objective in the triplet loss function to  get the distance between and an Anchor image and a positive or a negative image.\n",
    "    - Positive means the same person, while negative means different person\n",
    "- Formally, we want\n",
    "    - Positive distance to be less than the negative distance\n",
    "    - To make sure that the NN won't get an output  zero/ set the encodings equal to each other, we let\n",
    "        ```|| f(A) - f(p)||^2 - ||f(A) - f(N)|| ^2 + alpha < = 0```\n",
    "- Final loss funcion:\n",
    "    - L(A,P,N) = max(|| f(A) - f(p)||^2 - ||f(A) - f(N)|| ^2 + alpha, 0): So long as you achived your goal , which is the first term is < 0, the lost is 0.\n",
    "    - J = sum(L(A(i),P(i),N(i))) for all triplets of images\n",
    "- During training, if A, P, N are chosen randomly, $d(A,P) + \\alpha < d(A,N)$ is easily satisfied.\n",
    "- Choose triplets that's hard to train on. \n",
    "- Available implementations for face recognition using deep learning includes:\n",
    "    - Openface\n",
    "    - Facenet\n",
    "    - Deepface\n",
    "- Another way to learn the parameters is pose the face recognization as a binary classification problem. Take a pair of image, and feed to the Siaseme network and output 1/0.\n",
    "\n",
    "### Neural Style Transfer\n",
    "- To visualize what a deep learning layer is learning, pick a unit in layer l. Find the nine image patches that maximize the unit's activation. \n",
    "- It turns out the shallow layers are learning low level features like edge or colors, while deeper layers are learning more complex feature. \n",
    "#### Cost function\n",
    "- Given a content image C, a style image S and a generated image G:\n",
    "    - J(G) = alpha * J(C,G) + beta J(S,G)\n",
    "    - $\\alpha, \\beta$ are relative weighting to the similarity and these are hyperparameters.\n",
    "    - In practice,only one hyperparameter is needed.\n",
    "- Find the generated image G:\n",
    "    - Initiate G randomly:\n",
    "    - Use a gradient descent to minimize J(G), G = G - dG\n",
    "    \n",
    "#### Content cost function\n",
    "- Say you use hidden layer l to computer content test:\n",
    "    - If we choose l to be really small, then it will really force your generated image to pixel values very similar to your cotent image. \n",
    "    - In practice l is not too small and not too deep but in the middle. \n",
    "- Use pre-trained ConvNet(E.g. VGG network)\n",
    "- Let a(C)[l] and a(G)[l] be the activation of layer l on the images\n",
    "- If a(C)[l] and a(G)[l] are similar then they will have the same content.\n",
    "    - J(C,G) at layer l = 1/2|| a(C)[l] - a(G)(l)||^2\n",
    "    \n",
    "#### Style Cost function\n",
    "- Say you are using layer l's activation to measure Style\n",
    "- Define style as correlation between activations across channels. \n",
    "- Correlation tells you which of these high level features/ texture compoenents detected by different channels  tend to occur or not to occure gogether.\n",
    "- More formally, given an image, calculate the Style matrix\n",
    "  - Let a(i,j,k)[l] = activation at (i,j,k). \n",
    "  - The style matrix G[l] has dimension n^{[l]}{_c} x n^{[l]}{_c} $. \n",
    "    \n",
    "    - ```G_{kk'}^{[l]} = \\sum _ {i=1}^{n_H^{[l]}} a_{ijk}^{[l]}a_{ijk'}^{[l]}```\n",
    "- More specifically, $G^{[l]}_{kk'}$ measure how correlated are the activations in channel k compared to the activations in channel $k'$ \n",
    "- G_{kk'} is big, means more correlation\n",
    "- It turns out that you get more visual pleasing results if you use the sytle cost function from multiple layers. So the overall style cost function \n",
    "- Steps to be made if you want to create a tensorflow model for neural style transfer:\n",
    "    - Create an Interactive Session\n",
    "    - Load the content image\n",
    "    - Load the style image\n",
    "    - Randomly initialize the image to be generated\n",
    "    - Load the VGG16 model\n",
    "    - Build the Tensorflow graph:\n",
    "        - Run the content image through the VGG16 model and compute the content cost\n",
    "        - Run the style image through the VGG16 model and compute the style cost\n",
    "        - Compute the total cost\n",
    "        - Define the optimizer and the learning rate\n",
    "- Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step.\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
