{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of a CNN\n",
    "## Convolution layer\n",
    "- One of the challenges of computer vision problems is that the input image can get really big, Thus in standard fully connected NN, the weight matrix has very large dimension. With so many parameters, it's difficult to get enough data to prevent a NN from overfitting. And also, the computational requirements and the memory requirements to train a NN is a bit infeasible. Thus, for computer vison to use large images, we need to implement the convolution operation.\n",
    "- In general, if a  $n\\times n$ matrix convolved with $f \\times f$ filter/kernel gives us $(n-f+1) \\times (n-f+1)$ matrix \n",
    "- In python, use function conv-forward; In tensorflow use tf.nn.conv2d; in keras use Conv2d function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- We want to apply convolution operation multiple times, but if the image shrinks we will lose a lot of data during this process. Also, the edges pixels are used less than other pixels in an image.\n",
    "- To solve these problems, we can pad the input image before convolution by adding some rows and columns to it. We will call the padding amount $p$ the number of row/columns that we will insert in top, bottom, left and right of the image. \n",
    "- The general rule now, if a matrix $n\\times n $ is convolved with $f\\times f $ filter/kernel and padding $p$ gives us $n+2p-f+1 \\times n+2p-f+1$ matrix. \n",
    "- In computer vision f is usually odd. Some of the reasons is that it will have a center value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride Convolution\n",
    "- When we are making the convolution operation we used $S$ to tell us the number of pixels we will jump when are convolving filter/kernel. \n",
    "- General rule: If a matrix $n\\times n $ is convolved with $f \\times f $ filter/kernel and padding $p$ and stride $s$ it gives us $(\\frac{n+2p-f}{s} +1) \\times(\\frac{n+2p-f}{s} +1) $ matrix\n",
    "- In case $\\frac{n+2p-f}{s} +1$ is fraction we can take floor of this value.\n",
    "- In math textbooks the conv operation is flipping the filter before using it. What we were doing here is called cross- correlation operation, but the state of art of deep learning is using this as conv operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions over volumes\n",
    "- A image of height , width, # of channels is convoluting with  a filter of height, width, and same # of channels. The image number channels and the filter number of channels are the same. The output is here is only 2D. \n",
    "- Different layers/channels in the filter can either be the same matirx or different.  \n",
    "- You can use several  volume filters in one step to detect different features, each of them is going to give a 2D matrix, and then stack the results together to form a 3D matrix. Thus, if a $n\\times n \\times n_c$ matrix convolves with $ n_c'$  filters with shape $f \\times f \\times n_c$, then we got a $(n-f) \\times (n-f) \\times n_c'$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "- CNN often uses pooling layers to reduce the size of the inputs, speed up computation, and to make some of the feature dectors more invariant to its positon in the iinput.  \n",
    "- The two types of pooling layers are: max pooling and average pooing. Max pooling is used much more often than average pooling. \n",
    "- The max pooling is saying, if the feature is detected anywhere in this filter then keep the highest number. But the main reason why people are using pooling is because it works well in practice and reduce computations, and nobody know exactly why pooling works.\n",
    "- Max pooling: slides an (f,f) window over the input and store the max value of the window in the output. Pooling is done on each layer/ chanel independently. \n",
    "- Just  like convolution, f and s are hyperparameters of pooling layer. Padding is rarely used. Most often, s=2, f=2, which is going to shrink the matrix size be a factor around 2. \n",
    "\n",
    "- Poolign layer has no parameters for backprop to train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One layer of convolutional network\n",
    "- We first convolve input with some filters and then add a bias to each filter, and then get RELU activation of the result. The filter plays the similar role as the weight matrix.\n",
    "- Summar of notation:\n",
    "    If layer l is a convolution layer:\n",
    "        - f[l] = filter size in layer l\n",
    "        - p[l] = padding \n",
    "        - s[l] = stride \n",
    "        - n_c[l] = number of filters\n",
    "        - Each filter is f[l] * f[l]* n[l-1]_c\n",
    "        \n",
    "        - Input: n^[l-1]_H * n^[l-1]_w * n^[l-1]_c\n",
    "        - Output: n^[l]_H * n^[l]_w * n^[l]_c$\n",
    "        - n^[l] _H = {n[l-1] + 2p[l] -f[l]}/s[l] +1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why convolutions\n",
    "- Two main advantages of Conv are:\n",
    "    - Parameter sharing\n",
    "        - A feature detector that's useful in one part of the image is probably useful in another part of the image\n",
    "    - Sparsity of connections\n",
    "        - In each layer, each ouput value depends only on a small number of inputs which makes it translation\n",
    "invariance.\n",
    "    Through these two mechanisms, a neural netowrk has a lot fewer parameters which allows it to be trained with smaller training cells and is less prone to be overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep convolutional models: case studies\n",
    "- Here are some classical CNN networks:\n",
    "    - LeNet-5\n",
    "    - AlexNet\n",
    "    - VGG\n",
    "- The best CNN architecture that won the last ImageNet competition is called ResNet and it has 152 layers. \n",
    "- There are also an architecture called Inception that was made by Google that are very useful to learn and apply to your tasks.\n",
    "  <img src=\"../Images/LeNet5.png\" />\n",
    "### Lenet-5\n",
    "- Invented by Young Lecun in 1998.\n",
    "- Some statistics about the example: \n",
    "    -n_c: # of filters\n",
    "\n",
    "\n",
    "|  Layer |                          |Output/Activation Shape |  # of parameters| \n",
    "|------  |-------                   |-------| \n",
    "| Input |                           |(32,32,3) | |\n",
    "|1|CONV1(f1=5,s1=1, p1=0,n_c = 6)| (28,28,6)|156|\n",
    "|  |MaxPooling(f1p =2, s1p=2)       | (14,14,6)|0|\n",
    "| 2 |CONV2(f2=5,s2=1,p2=0, n_c = 16)|  (10,10,16)|416|\n",
    "|    |MaxPooling(f2p = 2, s2p =2) | (5,5,16)|0|\n",
    " | 3| FC3(number of neurons 120)  |(120,1) |48001|\n",
    " | 4 | FC4(number of neurons 84)   | (84,1))|10081|\n",
    " |5| Softmax| (?,1)|?| |\n",
    "### Alexnet\n",
    "- The goal for the model was the ImageNet challenge which classfies images into 1000 classes. \n",
    "- Summary:\n",
    "    ``` Conv => Max-pool => Conv => Max-pool => Conv => Conv => Conv => Max-pool => Flatten => FC=> FC => Softmax\n",
    "    ```\n",
    "- The paper convinced the computer science researchers that deep learning is so important. \n",
    "###VGG-16\n",
    "- A modificatio of AlexNet\n",
    "- Focus on having only these blocks:\n",
    "    - CONV = 3x3 filters, s=1,same padding\n",
    "    - MAX-POOL = 2x2, s=2\n",
    "- Pooling was the only one who is responsible for shrinking the dimensions.\n",
    "## Residual Network(ResNets)\n",
    "- Very, very deep NNs are difficult to train because of vanishing and exploding gradients problems. \n",
    "- In ResNets, we skip connection which makes you take the activation from one layer and suddently feed it to another layer even much deeper in NN which allows you to train deeper NNs even with layers greater than 100. \n",
    "- Theoratically, deeper and deeper NN should lead to smaller and smaller training error, but in practice, because of the vanishing and exploding gradients problems that performance of the network suffers as it goes deeper. While ResNets allows for deeper NN while not hurting the performance. \n",
    "### Why ResNets work?\n",
    "- Identity function is easy for a residul block to learn, which means that adding these two layers/residual block in your NN, it doesn't really hurt yor neural network's ability to do as well as the simpler network without these two layers. Also, if all these residual blocks actually learned something useful then maybe you do even better. \n",
    "- Also, what goes wrong in very  deep palin nets without these residual blocks is that when you make the network deeper and deeper, it's actually very difficutlt for it to choose parameters that learn even the identity function which is why a lot of layers end up making your result worse. \n",
    "- The main reason that residul network works is that it's so easy for these extra layers to learn the identity function that you're kind of guaranteed that it doesn't hurt performance and then a lot of the time you maybe be lucky and then even helps performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 X 1 convolutions/ Network in Network\n",
    "- The idea of one by one convolution has been very influential.For example:\n",
    "    - To shrink the number of channels and therefore save on computations, this is also called feature transformation, while pooing layer only shring $n_w, n_h$\n",
    "    - If you want to keep the number of channels, that's fine too. The effect of the 1X1 convolution is just adds non-linearity. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception network\n",
    "- When designing a CNN, you have to deide all the layer, such as will you pick a 3x3 Conv or 5x5 Conv or maybe a maxplooling layer. You have so many choices.\n",
    "- What inception tells us is, why not use all of them at once? Do the pooing, convs and then stack them together to form a volume output. \n",
    "- But the computation is costy in Inception model, but we can use a 1x1 convolution to reduce the computation. The 1x1 Conv here is called BottleNeck. \n",
    "- It turns out that 1X1 convolution won't hurt the performance. \n",
    "- Inception module\n",
    "\n",
    "- Inception network is the inception module repeatd several times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transfer Learning\n",
    "- If you are using a specific NN architecture that has been trained before, you can use this pretrained parameters instead of random initialization to solve your problems. \n",
    "- Frameworks have options to make the parameters frozen in some layers using ```trainable = 0 ``` or ```freeze =0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "- The more data you have, the better your deep NN's performance. Data augmentation is one of the techniques that deep learning uses to improve the performance of deep NN. \n",
    "- Some data augumentatio methods that are used for CV tasks includes:\n",
    "    - Mirroring\n",
    "    - Random cropping\n",
    "        - The issue with this technique is that you might take a wrong crop, and the solution is to make your crop big enough .\n",
    "    - Color shifting\n",
    "        - For example, we add tO R,G and B channels different distortions that will make the image identified as the same for the human but is different for the computer. \n",
    "        - In practice, the added value are pulled from some probability distribution and these shifts are quite small. \n",
    "        - Makes your algorithm more robust in changing colors in image. \n",
    "        - There are an algorithm which is called PCA color augmentation that decides the shifts needed automatically, and it is given in AlexNet paper. \n",
    "    - Rotation\n",
    "    - Shearing\n",
    "    - Local warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of Computer Vision\n",
    "- Speech recognition problems for example has a big amount of data, while image recognition has a medium amount of data and the object detection has a small amount of data nowadays.\n",
    "- If your problem has a large amount of data, researchers are tend to use:\n",
    "    - Simpler algorithms.\n",
    "    - Less hand engineering\n",
    "- If you don't have that much data people tend to try more hand engineering for the problem \"Hacks\". Like choosing a more complex NN architecture\n",
    "- Learning algorithms has two sources of knowledge:\n",
    "        - (x,y)labels \n",
    "        - hand engineered features/network architectures/other components. \n",
    "- Tips for doing well on benchmarks/winning competitions:\n",
    "    - Ensembling\n",
    "        - Train several networks independently and average their outputs. \n",
    "\n",
    "        - This can give you a push by 2%\n",
    "        - But this will slow down your production by the number of the ensembles. Also it takes more memory as it saves all the models in the memory.\n",
    "        - People use this in competitions but few uses this in a real production.\n",
    "    - Multi crop at test time\n",
    "        - Do data augumentation on test data as well \n",
    "        - Run classifier on multiple versions of test versions and average results.\n",
    "    - Use open source code\n",
    "         - Use architectures of networks published in the literature.\n",
    "         - Use open source implementations if possible.\n",
    "          - Use pretrained models and fine-tune on your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
